{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook demonstrates how you can run my pre-trained model on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My virtual environment is tracked using `pipenv`.\n",
    "# From the top directory of the project, run:\n",
    "!pipenv install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "\n",
    "from functools import partial\n",
    "import tqdm, os, json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gvpgnn.datasets as datasets\n",
    "import gvpgnn.models as models\n",
    "import gvpgnn.paths as paths\n",
    "import gvpgnn.data_models as dm\n",
    "import gvpgnn.embeddings as embeddings\n",
    "import gvpgnn.train_utils as train_utils\n",
    "import numpy as np\n",
    "import torch_geometric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scripts.parser import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required files:\n",
    "- A CSV with the same format as `cath_w_seqs_share.csv` that contains test data (the exact filename can be changed below)\n",
    "- A folder like `pdb_share` containing PDB files (the exact folder can be changed below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a: Preprocess the Dataset\n",
    "\n",
    "For convenience, I map the provided raw data to a format that's easier for my dataloader to use. You'll need to preprocess any unseen test data in the same way by following the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the top level of the repo:\n",
    "!cd scripts/\n",
    "!python preprocess.py \\\n",
    "  --csv path_to_your_test_data.csv \\\n",
    "  --output-folder ../data/challenge_test_set \\\n",
    "  --pdb-folder ../data/pdb_share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: Pre-Compute Language Model Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I precompute language model embeddings for all of the examples in the dataset. These are placed alongside the `JSON` data as `.pt` files. The whole dataset is copied to a new folder to avoid overwriting any of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs a script that fetches the pre-trained weights for all language models:\n",
    "!cd scipts/\n",
    "!python download_esm.py\n",
    "\n",
    "# Then run a script to precompute the embeddings:\n",
    "!python precompute_embeddings.py --in-dataset ../data/challenge_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the Model on the Test Set\n",
    "\n",
    "With the pre-processed test dataset, you can run the `train.py` script in test mode.\n",
    "\n",
    "You should see a progress bar appear as batches are processed.\n",
    "\n",
    "Finally, you should get a printout of metrics like the following:\n",
    "```bash\n",
    "*** Test set results:\n",
    "Loss: 1.4339 acc: 0.5044\n",
    "Accuracy (top1): 50.4362%\n",
    "----------------------------------------------------\n",
    "\n",
    "\t(1, 10)\t(1, 20)\t(2, 30)\t(2, 40)\t(2, 60)\t(3, 10)\t(3, 20)\t(3, 30)\t(3, 40)\t(3, 90)\tCount\n",
    "(1, 10)\t659\t265\t0\t0\t0\t0\t0\t68\t0\t8\t132\n",
    "(1, 20)\t287\t704\t0\t0\t0\t0\t0\t9\t0\t0\t115\n",
    "(2, 30)\t0\t0\t39\t305\t367\t109\t0\t180\t0\t0\t128\n",
    "(2, 40)\t0\t0\t0\t436\t257\t57\t0\t193\t0\t57\t140\n",
    "(2, 60)\t0\t0\t0\t31\t915\t15\t0\t38\t0\t0\t130\n",
    "(3, 10)\t0\t0\t0\t0\t0\t108\t0\t892\t0\t0\t139\n",
    "(3, 20)\t0\t0\t0\t0\t0\t0\t1000\t0\t0\t0\t104\n",
    "(3, 30)\t44\t0\t0\t0\t88\t88\t0\t555\t182\t44\t137\n",
    "(3, 40)\t23\t30\t0\t0\t0\t8\t23\t197\t667\t53\t132\n",
    "(3, 90)\t0\t0\t0\t0\t731\t269\t0\t0\t0\t0\t104\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd scripts\n",
    "!./test_best_model.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cath-proteins-q1ibxFh1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
