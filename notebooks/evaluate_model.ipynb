{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook demonstrates how you can run my pre-trained model on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My virtual environment is tracked using `pipenv`.\n",
    "# From the top directory of the project, run:\n",
    "!pipenv install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "\n",
    "from functools import partial\n",
    "import tqdm, os, json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gvpgnn.datasets as datasets\n",
    "import gvpgnn.models as models\n",
    "import gvpgnn.paths as paths\n",
    "import gvpgnn.data_models as dm\n",
    "import gvpgnn.embeddings as embeddings\n",
    "import gvpgnn.train_utils as train_utils\n",
    "import numpy as np\n",
    "import torch_geometric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scripts.parser import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, I map the provided raw data to a format that's easier for my dataloader to use. You'll need to preprocess any unseen test data in the same way.\n",
    "\n",
    "\n",
    "### Required files:\n",
    "- I'm expecting the TEST data to be found in a CSV with the same format as `cath_w_seqs_share.csv` (filename can be changed below)\n",
    "- I'm expecting the unseen proteins to have PDB files in a folder like `pdb_share` (folder can be changed below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a: Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the top level of the repo:\n",
    "!cd scripts/\n",
    "!python preprocess.py \\\n",
    "  --csv path_to_your_test_data.csv \\\n",
    "  --output-folder ../data/challenge_test_set \\\n",
    "  --pdb-folder ../data/pdb_share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: Pre-Compute Language Model Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I precompute language model embeddings for all of the examples in the dataset. These are placed alongside the `JSON` data as `.pt` files. The whole dataset is copied to a new folder to avoid overwriting any of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs a script that fetches the pre-trained weights for all language models:\n",
    "!cd scipts/\n",
    "!python download_esm.py\n",
    "\n",
    "# Then run a script to precompute the embeddings:\n",
    "!python precompute_embeddings.py --in-dataset ../data/challenge_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cath-proteins-q1ibxFh1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
