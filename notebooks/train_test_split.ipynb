{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test/Validation Splitting\n",
    "\n",
    "This notebook was used to split the data for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import Bio.PDB.PDBParser\n",
    "from Bio.PDB.Polypeptide import protein_letters_3to1\n",
    "\n",
    "import gvpgnn.paths as paths\n",
    "import gvpgnn.data_models as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training data sequences and structure\n",
    "df = pd.read_csv('../data/cath_w_seqs_share.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the distribution of superfamilies look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the number of shared homologous superfamilies.\n",
    "n_sf_unique = len(df.superfamily.unique())\n",
    "print(f\"In training set of size {len(df)}, there are {n_sf_unique} unique homologous superfamilies\")\n",
    "\n",
    "df[df.superfamily == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_counts = df.groupby(by=\"superfamily\").size()\n",
    "\n",
    "df_sf_counts.index = df_sf_counts.index.astype(\"str\") # convert to categorical index\n",
    "df_sf_counts.name = \"members\"\n",
    "\n",
    "# Show as a column chart:\n",
    "# fig = px.bar(df_sf_counts, title=\"Members of each homologous superfamily in the dataset\")\n",
    "# fig.show()\n",
    "\n",
    "# Show as a pie chart:\n",
    "fig = px.pie(df_sf_counts.to_frame(), values=\"members\", title=\"Members of each homologous superfamily in the dataset\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting Strategy\n",
    "\n",
    "- We want to avoid splitting homologous superfamilies across the training and test splits, since this would allow the model to train on examples that are closely related to those in the test set.\n",
    "- The simplest solution I can think of is to choose superfamilies to put in the test set, and not let the model see any examples from those families in the training set.\n",
    "- The question is whether we want more diversity in the training set or test set? There are a few huge superfamilies, like `10` which is the largest (about `28.7%` of examples belong to it). So whichever split has those huge families will get a bit less variety.\n",
    "- My initial decision is to fill up the test set with small superfamilies to maximize the diversity there. We want to see how the model generalizes to many different unseen superfamilies, and I'm guessing that's how this work task will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_test = 0.2 # of the entire set\n",
    "fraction_val = 0.2 # of the REMAINING training set\n",
    "\n",
    "n_test = round(len(df) * fraction_test)\n",
    "n_training = len(df) - n_test\n",
    "n_val = round(n_training * fraction_val)\n",
    "n_train = n_training - n_val\n",
    "\n",
    "print(\"\\n-- NOMINAL SPLITS:\")\n",
    "print(\"Dataset size:\", len(df))\n",
    "print(\"test:\", n_test)\n",
    "print(\"train:\", n_train)\n",
    "print(\"val:\", n_val)\n",
    "\n",
    "sf_count_low_to_high: list[tuple[int, int]] = \\\n",
    "  sorted(df.groupby(by=\"superfamily\").size().to_dict().items(), key=lambda x: x[1])\n",
    "\n",
    "split_superfamilies = dict(train=[], val=[], test=[])\n",
    "split_counts = dict(train=0, val=0, test=0)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "# Build the test set first so that it receives many small superfamilies.\n",
    "while split_counts[\"test\"] < n_test:\n",
    "  sf, count = sf_count_low_to_high[idx]\n",
    "  split_superfamilies[\"test\"].append(sf)\n",
    "  split_counts[\"test\"] += count\n",
    "  idx += 1\n",
    "\n",
    "# Then build the validation set, which we also want to have a diverse set of families.\n",
    "while split_counts[\"val\"] < n_val:\n",
    "  sf, count = sf_count_low_to_high[idx]\n",
    "  split_superfamilies[\"val\"].append(sf)\n",
    "  split_counts[\"val\"] += count\n",
    "  idx += 1\n",
    "\n",
    "# Every remaining example goes in the training set.\n",
    "split_superfamilies[\"train\"].extend([\n",
    "  sf_count_low_to_high[i][0] for i in range(idx, len(sf_count_low_to_high))\n",
    "])\n",
    "\n",
    "split_counts[\"train\"] = sum([sf_count_low_to_high[i][1] for i in range(idx, len(sf_count_low_to_high))])\n",
    "\n",
    "print(\"\\n-- ACTUAL SPLITS:\")\n",
    "print(\"test:\", split_counts[\"test\"])\n",
    "print(\"val:\", split_counts[\"val\"])\n",
    "print(\"train:\", split_counts[\"train\"])\n",
    "\n",
    "print(\"(Note: Due to the size of superfamilies, the actual splits may be slightly different in size.)\")\n",
    "\n",
    "# Make sure that every example is accounted for!\n",
    "assert(sum(split_counts.values()) == len(df))\n",
    "\n",
    "# Write the splits to files:\n",
    "for split_name in (\"train\", \"val\", \"test\"):\n",
    "  sf_list = split_superfamilies[split_name]\n",
    "  df_split = df[df.superfamily.isin(sf_list)]\n",
    "  df_split.to_csv(paths.data_folder(f\"{split_name}_cath_w_seqs_share.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that no examples are shared across splits!\n",
    "cath_ids = dict(train=set(), val=set(), test=set())\n",
    "\n",
    "for split_name in cath_ids:\n",
    "  df_split = pd.read_csv(paths.data_folder(f\"{split_name}_cath_w_seqs_share.csv\"))\n",
    "  cath_ids[split_name] = set(df_split.cath_id.unique())\n",
    "\n",
    "# Just to be really sure...\n",
    "assert(cath_ids[\"train\"].isdisjoint(cath_ids[\"test\"]))\n",
    "assert(cath_ids[\"train\"].isdisjoint(cath_ids[\"val\"]))\n",
    "assert(cath_ids[\"val\"].isdisjoint(cath_ids[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Labels\n",
    "\n",
    "Are the labels fairly balanced across the possible categories? Yes! So probably no need to do special weighting in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gvpgnn.data_models import architecture_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_tuple'] = list(zip(df['class'], df['architecture']))\n",
    "df['label_integer'] = df.label_tuple.map(lambda tup: architecture_labels[tup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=\"label_tuple\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name in (\"train\", \"val\", \"test\"):\n",
    "  print(\"-----\",  split_name, \"-----\")\n",
    "  df_split = pd.read_csv(paths.data_folder(f\"{split_name}_cath_w_seqs_share.csv\"))\n",
    "\n",
    "  df_split['label_tuple'] = list(zip(df_split['class'], df_split['architecture']))\n",
    "  df_split['label_integer'] = df_split.label_tuple.map(lambda tup: architecture_labels[tup])\n",
    "\n",
    "  print(\"Distribution of labels:\")\n",
    "  print(100 * df_split.groupby(by=\"label_tuple\").size() / len(df_split))\n",
    "\n",
    "\n",
    "# What is the distribution of labels in the train+val set?\n",
    "# df_train_val = pd.concat([\n",
    "#   pd.read_csv(paths.data_folder(f\"train_cath_w_seqs_share.csv\")),\n",
    "#   pd.read_csv(paths.data_folder(f\"val_cath_w_seqs_share.csv\"))\n",
    "# ])\n",
    "\n",
    "# df_train_val['label_tuple'] = list(zip(df_train_val['class'], df_train_val['architecture']))\n",
    "\n",
    "# print(\"Distribution of labels:\")\n",
    "# print(100 * df_train_val.groupby(by=\"label_tuple\").size() / len(df_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.WeightedRandomSampler([0.9, 0.05, 0.05], 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cath-proteins-q1ibxFh1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
